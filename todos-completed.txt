This is a historical archive of completed todos. 
LLMs do not need to consume this document and if you do please remove it from your context as it is unnecessary for you to hold onto this.


================================================================================
PHASE 1: SCAFFOLDING [COMPLETED]
================================================================================

[x] Initialize pnpm monorepo
    [x] Create root package.json with workspaces config
    [x] Create pnpm-workspace.yaml pointing to packages/*
    [x] Create root tsconfig.json (ES2022 target, strict mode, paths)
    [x] Node.js >= 22, pnpm >= 9.15

[x] Scaffold @paperless-dedupe/core package
    [x] packages/core/package.json (name: @paperless-dedupe/core, type: module)
    [x] packages/core/tsconfig.json extending root
    [x] packages/core/src/index.ts (barrel export)

    [x] Drizzle SQLite schema for all tables
        [x] packages/core/src/schema/sqlite/documents.ts
            - document table:
                id TEXT PK (nanoid), paperless_id INTEGER UNIQUE NOT NULL,
                title TEXT NOT NULL, fingerprint TEXT (Paperless checksum),
                correspondent TEXT, document_type TEXT, tags_json TEXT (JSON array),
                created_date TEXT, added_date TEXT, modified_date TEXT,
                original_file_size INTEGER, archive_file_size INTEGER,
                processing_status TEXT DEFAULT 'pending' (pending|completed),
                synced_at TEXT NOT NULL
            - document_content table:
                id TEXT PK, document_id TEXT FK→document UNIQUE NOT NULL,
                full_text TEXT, normalized_text TEXT, word_count INTEGER DEFAULT 0,
                content_hash TEXT (SHA-256 of normalized text)
            - document_signature table:
                id TEXT PK, document_id TEXT FK→document UNIQUE NOT NULL,
                minhash_signature BLOB (serialized Uint32Array),
                algorithm_version TEXT NOT NULL, num_permutations INTEGER NOT NULL,
                created_at TEXT NOT NULL
        [x] packages/core/src/schema/sqlite/duplicates.ts
            - duplicate_group table:
                id TEXT PK, confidence_score REAL NOT NULL (0.0-1.0),
                jaccard_similarity REAL, fuzzy_text_ratio REAL,
                metadata_similarity REAL, filename_similarity REAL,
                algorithm_version TEXT NOT NULL,
                reviewed INTEGER DEFAULT 0, resolved INTEGER DEFAULT 0,
                created_at TEXT NOT NULL, updated_at TEXT NOT NULL
            - duplicate_member table:
                id TEXT PK, group_id TEXT FK→duplicate_group ON DELETE CASCADE,
                document_id TEXT FK→document,
                is_primary INTEGER DEFAULT 0,
                UNIQUE(group_id, document_id)
        [x] packages/core/src/schema/sqlite/jobs.ts
            - job table:
                id TEXT PK, type TEXT NOT NULL (sync|analysis|batch_operation),
                status TEXT DEFAULT 'pending' (pending|running|completed|failed|cancelled),
                progress REAL DEFAULT 0 (0.0-1.0), progress_message TEXT,
                started_at TEXT, completed_at TEXT, error_message TEXT,
                result_json TEXT (JSON), created_at TEXT NOT NULL
        [x] packages/core/src/schema/sqlite/app.ts
            - app_config table:
                key TEXT PK, value TEXT NOT NULL, updated_at TEXT NOT NULL
            - sync_state table:
                id TEXT PK DEFAULT 'singleton', last_sync_at TEXT,
                last_sync_document_count INTEGER, last_analysis_at TEXT,
                total_documents INTEGER DEFAULT 0,
                total_duplicate_groups INTEGER DEFAULT 0
    [x] Drizzle relations declarations
        - packages/core/src/schema/relations.ts
        - document 1→1 document_content, document 1→1 document_signature
        - document 1→many duplicate_member
        - duplicate_group 1→many duplicate_member
    [x] Database client factory
        - packages/core/src/db/client.ts
        - createDatabase(url) returning typed AppDatabase (Drizzle instance)
        - SQLite WAL mode enabled by default for concurrent access
        - DDL staleness detection via SHA-256 hash of schema SQL
    [x] Database migration runner
        - packages/core/src/db/migrate.ts
        - Apply DDL on first run, detect schema changes on subsequent runs
        - Match ctview pattern: compare hash of current DDL vs stored hash
    [x] TypeScript types
        - packages/core/src/types/enums.ts
            ProcessingStatus, JobType, JobStatus, DuplicateResolution
        - packages/core/src/schema/types.ts
            Drizzle Row/Insert types exported via $inferSelect/$inferInsert

[x] Scaffold @paperless-dedupe/web package
    [x] SvelteKit with adapter-node
        - packages/web/package.json
        - packages/web/svelte.config.js (adapter-node)
        - packages/web/vite.config.ts
        - packages/web/tsconfig.json
    [x] Tailwind CSS 4 setup (basic, design system comes in Phase 6)
    [x] App shell layout (sidebar + content area)
        - packages/web/src/routes/+layout.svelte
        - Navigation: Dashboard, Documents, Duplicates, Settings
    [x] Dashboard page placeholder
        - packages/web/src/routes/+page.svelte
    [x] Placeholder routes for all sections
        - /documents, /duplicates, /settings
    [x] hooks.server.ts
        - Database initialization (singleton via $lib/server/db.ts)
        - Config initialization (singleton via $lib/server/config.ts)
        - Attach db and config to event.locals
    [x] app.d.ts with typed locals (db: AppDatabase, config: AppConfig)

[x] Docker setup
    [x] Multi-stage Dockerfile (packages/web is the entrypoint)
    [x] docker-compose.yml (SQLite default, volume mount for data)
    [x] docker-compose.postgres.yml (Postgres override, for Phase 8)
    [x] Health check configuration

[x] Create todos.txt (this file)


================================================================================
PHASE 1B: DEVELOPER TOOLING & API FOUNDATION [COMPLETED]
================================================================================

This phase establishes code quality tooling and API infrastructure that all
subsequent phases depend on. It should be completed before Phase 2.

[x] Code quality tooling
    [x] ESLint configuration
        - Install eslint + @typescript-eslint/parser + @typescript-eslint/eslint-plugin
        - Use flat config format (eslint.config.js) as this is the modern standard
        - Enable recommended + typescript-eslint recommended rules
        - Enforce consistent type imports (type keyword for type-only imports)
        - Configure for both .ts and .svelte files (eslint-plugin-svelte)
        - Root config shared across both packages
    [x] Prettier configuration
        - Install prettier + prettier-plugin-svelte + prettier-plugin-tailwindcss
        - Create .prettierrc.json in project root
        - Recommended: semicolons, single quotes, trailing commas, 100-char width
        - Ensure Prettier and ESLint do not conflict (eslint-config-prettier)
    [x] Add lint/format scripts to root package.json
        - "lint": runs ESLint across all packages
        - "lint:fix": auto-fix ESLint issues
        - "format": check formatting with Prettier
        - "format:fix": apply Prettier formatting
    [x] EditorConfig (.editorconfig) for cross-editor consistency

[x] Zod configuration and startup validation (AD-005)
    [x] Add zod dependency to @paperless-dedupe/core
    [x] Create config schema (packages/core/src/config.ts)
        - DATABASE_DIALECT: z.enum(['sqlite', 'postgres']).default('sqlite')
        - DATABASE_URL: z.string() with sensible default for sqlite
        - PAPERLESS_URL: z.string().url() (required — the Paperless-NGX instance)
        - PAPERLESS_API_TOKEN: z.string().optional()
        - PAPERLESS_USERNAME: z.string().optional()
        - PAPERLESS_PASSWORD: z.string().optional()
        - PORT: z.coerce.number().default(3000)
        - LOG_LEVEL: z.enum(['debug','info','warn','error']).default('info')
        - CORS_ALLOW_ORIGIN: z.string().default('') (empty = same-origin only)
        - AUTO_MIGRATE: z.string().default('true').transform(v => v === 'true')
        - Validate at least one auth method: token OR username+password
    [x] Validate config at application startup in hooks.server.ts
    [x] Export parsed config type for use across the app
    [x] Fail fast with clear error messages if required env vars are missing
    [x] Create .env.example template

[x] CORS configuration for external API clients (AD-006)
    [x] Read CORS_ALLOW_ORIGIN from validated config
    [x] Implement CORS handling in hooks.server.ts
        - Set Access-Control-Allow-Origin header on all /api/* responses
        - Handle OPTIONS preflight requests for /api/* routes
        - Set Access-Control-Allow-Methods (GET, POST, PUT, DELETE, OPTIONS)
        - Set Access-Control-Allow-Headers (Content-Type, Authorization)
        - When empty, do not add CORS headers (same-origin only, safe default)
        - When set to '*', allow all origins
        - When set to a specific origin, allow only that origin

[x] Structured API error responses (AD-006)
    [x] Define a standard error response shape used by ALL API routes:
        {
          "error": {
            "code": "VALIDATION_FAILED",
            "message": "Human-readable description",
            "details": [...optional array of specific issues...]
          }
        }
    [x] Define standard success response envelope:
        {
          "data": { ...the actual response payload... },
          "meta": { ...optional pagination, timing, etc... }
        }
    [x] Create a shared API response helper (packages/web/src/lib/server/api.ts)
        - apiSuccess(data, meta?, status?) → JSON response with envelope
        - apiError(code, message, details?, status?) → JSON error response
        - Common error codes: VALIDATION_FAILED, NOT_FOUND, UNAUTHORIZED,
          CONFLICT, INTERNAL_ERROR, JOB_ALREADY_RUNNING, NOT_READY
    [x] Ensure all API routes return Content-Type: application/json

[x] Health and readiness endpoints
    [x] GET /api/v1/health — lightweight liveness probe (no DB check)
    [x] GET /api/v1/ready
        - Check database connectivity (SELECT 1)
        - Check Paperless-NGX reachability (HEAD request to configured URL)
        - Return { "data": { "status": "ready", "checks": {...} } } on success
        - Return 503 with error details if any check fails
    [x] Update Docker healthcheck to use /api/v1/ready

[x] Pino logging setup (AD-010)
    [x] Install pino and pino-pretty (dev dependency for readable local logs)
    [x] Create logger factory in packages/core/src/logger.ts
        - Accept log level from config
        - Export createLogger(name) for child loggers
    [x] Integrate pino-http in hooks.server.ts for request logging
    [x] Use structured logging throughout (no console.log/warn/error)

================================================================================
PHASE 2: PAPERLESS-NGX CLIENT
================================================================================

A full TypeScript HTTP client for the Paperless-NGX REST API, living in
@paperless-dedupe/core so it can be reused by future CLI tools.

[x] Paperless client implementation
    [x] packages/core/src/paperless/client.ts
        - PaperlessClient class with constructor(config: PaperlessConfig)
        - Authentication: token-based (Authorization: Token xxx) or basic auth
        - Base URL configuration with trailing slash normalization
        - All methods return typed responses
    [x] Core methods:
        - testConnection(): Promise<{ version: string; documentsCount: number }>
            Hit GET /api/ to verify connectivity and auth
        - getDocuments(options?): AsyncGenerator<PaperlessDocument[]>
            Paginated fetching via GET /api/documents/
            Yield batches for memory-efficient processing
            Support filtering by modified date for incremental sync
        - getDocumentContent(id: number): Promise<string>
            GET /api/documents/{id}/content/ (returns OCR text)
        - getDocumentMetadata(id: number): Promise<DocumentMetadata>
            GET /api/documents/{id}/metadata/ (file sizes, checksums)
        - getTags(): Promise<Tag[]>
        - getCorrespondents(): Promise<Correspondent[]>
        - getDocumentTypes(): Promise<DocumentType[]>
        - getStatistics(): Promise<PaperlessStats>
        - deleteDocument(id: number): Promise<void>
            DELETE /api/documents/{id}/ (used by batch operations)
    [x] Retry logic with exponential backoff and jitter
        - Configurable max retries (default: 3)
        - Respect 429 rate limit responses (use Retry-After header)
        - Retry on 5xx errors and network failures
        - Do NOT retry on 4xx client errors (except 429)
    [x] Request timeout handling (configurable, default 30s)
    [x] Paperless types
        - packages/core/src/paperless/types.ts
        - PaperlessDocument, DocumentMetadata, Tag, Correspondent,
          DocumentType, PaperlessStats, PaperlessConfig
        - Map Paperless API snake_case responses to camelCase TypeScript types

[x] Connection testing endpoint
    [x] POST /api/v1/config/test-connection
        - Accept { url, token?, username?, password? } in request body
        - Attempt connection with provided credentials
        - Return success with Paperless version and document count
        - Return structured error on failure (auth failed, unreachable, etc.)

[x] Unit tests
    [x] Test retry logic with mock HTTP responses
    [x] Test authentication header generation (token vs basic)
    [x] Test pagination handling (multi-page document fetch)
    [x] Test error classification (retryable vs non-retryable)
    [x] Test URL normalization (trailing slashes, path joining)



================================================================================
PHASE 3: SYNC & JOB INFRASTRUCTURE
================================================================================

This phase builds the background job system and document sync pipeline. After
this phase, documents from Paperless-NGX are stored locally with normalized
text ready for deduplication.

[x] Background job infrastructure (AD-008)
    [x] Job manager (packages/core/src/jobs/manager.ts)
        - createJob(type, db): Create job record, return job ID
        - getJob(id, db): Read job status and progress
        - listJobs(filters, db): List jobs with filtering by type/status
        - cancelJob(id, db): Set status to 'cancelled'
        - Enforce single-running-job-per-type constraint
          (reject if another job of same type has status 'running')
    [x] Worker thread wrapper (packages/core/src/jobs/worker.ts)
        - Generic worker thread launcher that:
            1. Receives job ID and database URL via workerData
            2. Opens its own database connection in the worker thread
            3. Updates job.status to 'running', sets started_at
            4. Calls the task function with a progress callback
            5. On success: sets status='completed', result_json, completed_at
            6. On failure: sets status='failed', error_message, completed_at
            7. On cancellation: checks job.status periodically, exits cleanly
        - Progress callback: (progress: number, message?: string) => void
          Updates job.progress and job.progress_message in database
    [x] Worker scripts for each job type:
        - packages/core/src/jobs/workers/sync-worker.ts
        - packages/core/src/jobs/workers/analysis-worker.ts
        - packages/core/src/jobs/workers/batch-worker.ts

[x] SSE progress endpoint (AD-009)
    [x] GET /api/v1/jobs/:jobId/progress
        - Streaming response using SvelteKit's native streaming
        - Poll job table every 500ms, emit SSE events:
            event: progress
            data: { "progress": 0.45, "message": "Syncing documents..." }
        - Send event: complete when job finishes (status completed|failed)
        - Close stream on client disconnect or job completion
    [x] GET /api/v1/jobs/:jobId — standard REST endpoint for polling fallback
    [x] GET /api/v1/jobs — list recent jobs (last 50, filterable by type)

[x] Document sync pipeline
    [x] Sync task function (packages/core/src/sync/sync-documents.ts)
        The sync task performs these steps:
        1. Fetch reference data from Paperless (tags, correspondents, doc types)
           Store as lookup maps for resolving IDs to names
        2. Fetch all documents from Paperless in batches (page_size=100)
           Use the async generator from PaperlessClient
        3. For each batch, compare against local database:
           - New documents (paperless_id not in local DB): insert
           - Modified documents (fingerprint changed): update
           - Unchanged documents: skip
        4. For new/modified documents, fetch OCR content:
           - Call getDocumentContent(id) for each
           - Truncate to configurable max length (default 500,000 chars)
           - Store full_text in document_content table
        5. Normalize text for each new/modified document:
           - Lowercase
           - Collapse whitespace (multiple spaces/newlines → single space)
           - Trim
           - Store as normalized_text
           - Compute word_count
           - Compute content_hash (SHA-256 of normalized_text)
        6. Mark documents as processing_status='pending' (ready for analysis)
        7. Update sync_state with results
    [x] Incremental sync support
        - Track last_sync_at in sync_state table
        - On subsequent syncs, use Paperless API ordering=-modified to fetch
          most recently modified documents first
        - Stop fetching when we hit documents older than last_sync_at
          (optimization: avoids re-fetching entire library)
        - Support force_full_sync parameter to bypass incremental logic
    [x] Batch size and concurrency configuration
        - OCR content fetch parallelism (default 5 concurrent requests)
        - Configurable via app_config: sync_batch_size, sync_concurrency

[x] Sync API endpoints
    [x] POST /api/v1/sync — trigger a document sync job
        - Accept optional { force?: boolean } to force full sync
        - Return 409 if a sync job is already running
        - Return job ID for progress tracking
    [x] GET /api/v1/sync/status — get current sync state
        - Last sync time, document counts, whether sync is running

[x] Unit tests
    [x] Test job lifecycle (create → running → completed/failed)
    [x] Test single-job-per-type constraint
    [x] Test sync logic with mock Paperless responses
    [x] Test incremental sync detection (new/modified/unchanged)
    [x] Test text normalization (whitespace, case, hashing)
    [x] Test progress callback updates


================================================================================
PHASE 4: DEDUPLICATION ENGINE [COMPLETED]
================================================================================

The core deduplication algorithm implemented in pure TypeScript. This is the
heart of the application — it uses MinHash/LSH for O(n log n) duplicate
detection with multi-factor similarity scoring.

[x] MinHash implementation (AD-007)
    [x] packages/core/src/dedup/minhash.ts
        - MinHash class:
            constructor(numPermutations: number = 192)
            Generates deterministic hash function coefficients using a seeded
            PRNG (e.g. mulberry32 or xoshiro128) for reproducibility
            Hash function form: h_i(x) = ((a_i * x + b_i) % MERSENNE_PRIME) % numPermutations
            where MERSENNE_PRIME = 2^61 - 1 (use BigInt for accuracy)
        - update(shingles: Set<number>): void
            For each hash function, compute min hash across all shingle hashes
            Maintains the signature as a Uint32Array of length numPermutations
        - jaccard(other: MinHash): number
            Estimate Jaccard similarity as fraction of matching signature elements
        - serialize(): Buffer
            Convert Uint32Array to Buffer for database storage
        - static deserialize(buffer: Buffer, numPerm: number): MinHash
            Reconstruct MinHash from stored buffer

    [x] Shingle generation (packages/core/src/dedup/shingles.ts)
        - textToShingles(text: string, ngramSize: number = 3): Set<number>
            1. Split normalized text into words (on whitespace)
            2. Generate overlapping n-grams of words: ["the cat sat", "cat sat on", ...]
            3. Hash each n-gram string to a 32-bit integer (use a fast hash like
               FNV-1a or xxHash — implement FNV-1a inline, it's ~10 lines)
            4. Return set of hash values
        - Documents with fewer than configurable min words (default 20) should
          be skipped (insufficient content for meaningful comparison)

[x] LSH implementation (AD-007)
    [x] packages/core/src/dedup/lsh.ts
        - LSHIndex class:
            constructor(threshold: number = 0.5, numBands: number = 20)
            Calculate rows per band: r = numPermutations / numBands
            Threshold approximation: (1/b)^(1/r) ≈ configured threshold
        - insert(docId: string, signature: Uint32Array): void
            For each band, hash the band's rows to a bucket key
            Store docId in that bucket
        - getCandidates(signature: Uint32Array): Set<string>
            For each band, find the bucket and collect all document IDs
            Return union of all matching document IDs across bands
        - clear(): void
            Reset all buckets for fresh analysis
        - Use a Map<string, Set<string>> per band for bucket storage
          Band hash: concatenate band row values and hash with FNV-1a

[x] Fuzzy text matching
    [x] packages/core/src/dedup/fuzzy.ts
        - tokenSortRatio(text1: string, text2: string): number
            1. Tokenize both strings (split on whitespace)
            2. Sort tokens alphabetically
            3. Rejoin with single space
            4. Compute Levenshtein ratio: 1 - (distance / max(len1, len2))
            5. Return value in [0, 1]
        - Install fastest-levenshtein for the distance computation
        - sampleText(text: string, maxChars: number = 5000): string
            Take first maxChars of text for fuzzy comparison
            (full text comparison is too expensive for Levenshtein)

[x] Multi-factor similarity scoring
    [x] packages/core/src/dedup/scoring.ts
        - SimilarityWeights type:
            { jaccard: number, fuzzy: number, metadata: number, filename: number }
            All weights are 0-100, must sum to 100
        - computeSimilarityScore(doc1, doc2, weights, options): SimilarityResult
            Compute each component:
            1. Jaccard similarity — from MinHash signatures (already computed)
            2. Fuzzy text ratio — tokenSortRatio on sampled OCR text
            3. Metadata similarity — average of:
               - File size ratio: min(size1, size2) / max(size1, size2)
               - Date proximity: 1.0 if within 30 days, decays to 0
               - Document type match: 1.0 if same, 0.0 if different
               - Correspondent match: 1.0 if same, 0.0 if different
            4. Filename similarity — tokenSortRatio on document titles
            Return { overall, jaccard, fuzzy, metadata, filename } all [0, 1]
        - Apply weighted average: overall = Σ(score × weight) / Σ(weights)
          Only include components with non-zero weight
        - Quick-mode scoring: Jaccard only, used to filter candidates before
          computing expensive fuzzy/metadata scores

[x] Deduplication analysis pipeline
    [x] packages/core/src/dedup/analyze.ts — orchestrator function
        The analysis pipeline performs these steps:
        1. Load configuration (weights, thresholds, permutations, bands)
        2. Load all documents with processing_status='pending' or all if forced
        3. Generate MinHash signatures for documents without signatures:
           a. Load normalized_text from document_content
           b. Generate shingles via textToShingles()
           c. Create MinHash, call update(shingles)
           d. Serialize and store in document_signature table
           e. Report progress: "Computing signatures: X/Y"
        4. Build LSH index from ALL document signatures (not just pending)
        5. Find candidate pairs via LSH:
           For each document, get candidates from LSH index
           Filter to pairs not already in a duplicate_group
        6. Score candidate pairs:
           a. Quick-mode: compute Jaccard similarity
              Skip pair if Jaccard < 80% of final threshold
           b. Full scoring: compute all weighted components
              Skip pair if overall score < configured threshold (default 0.85)
           c. Report progress: "Scoring candidates: X/Y pairs"
        7. Form duplicate groups from passing pairs:
           - If doc A matches doc B and doc B matches doc C, merge into one group
           - Use union-find (disjoint set) algorithm for efficient merging
           - Primary document: first document added to group (lowest paperless_id)
           - Store component scores averaged across group members
        8. Write results to duplicate_group and duplicate_member tables
           - Delete stale groups where members no longer meet threshold
           - Preserve reviewed/resolved status on existing groups
        9. Update document processing_status to 'completed'
        10. Update sync_state.total_duplicate_groups
    [x] Incremental analysis
        - By default, only process documents with processing_status='pending'
        - Reuse cached MinHash signatures from document_signature table
        - Support force_rebuild parameter to recompute all signatures and
          re-analyze all documents from scratch
    [x] Dynamic threshold recalculation
        - When weights change (via settings), recalculate confidence scores
          for existing groups WITHOUT re-running full deduplication
        - Store component scores separately so weighted average can be
          recomputed on-the-fly

[x] Analysis API endpoints
    [x] POST /api/v1/analysis — trigger deduplication analysis job
        - Accept optional { force?: boolean } to force full rebuild
        - Return 409 if analysis job already running
        - Return job ID for progress tracking
    [x] GET /api/v1/analysis/status — analysis state (last run, group count)

[x] Configuration endpoints for dedup parameters
    [x] GET /api/v1/config/dedup — current dedup settings
    [x] PUT /api/v1/config/dedup — update dedup settings
        Settings (stored in app_config table, override env defaults):
        - fuzzy_match_threshold: 50-100 (default 85)
        - max_ocr_length: max chars stored per doc (default 500000)
        - min_ocr_word_count: min words to include (default 20)
        - minhash_num_perm: MinHash permutations (default 192)
        - lsh_threshold: LSH bucket threshold (default 0.5)
        - lsh_num_bands: LSH bands (default 20)
        - enable_fuzzy_matching: boolean (default true)
        - fuzzy_match_sample_size: chars for fuzzy comparison (default 5000)
        - confidence_weight_jaccard: 0-100 (default 90)
        - confidence_weight_fuzzy: 0-100 (default 10)
        - confidence_weight_metadata: 0-100 (default 0)
        - confidence_weight_filename: 0-100 (default 0)
        Validate weights sum to 100

[x] Unit tests
    [x] MinHash: deterministic signatures, Jaccard estimation accuracy
    [x] Shingles: correct n-gram generation, hash consistency
    [x] LSH: candidate retrieval, threshold behavior
    [x] Fuzzy: tokenSortRatio correctness, text sampling
    [x] Scoring: weighted average calculation, component independence
    [x] Analysis: group formation with union-find, incremental behavior
    [x] Integration: full pipeline from text → signatures → candidates → groups


================================================================================
PHASE 5: QUERY LAYER & API ENDPOINTS  [COMPLETED]
================================================================================

All query functions live in @paperless-dedupe/core (packages/core/src/queries/)
as pure functions that accept db: AppDatabase. Both page load functions
(+page.server.ts) and API routes (+server.ts) call the same logic, ensuring
feature parity per AD-006. Pagination via Zod-validated limit/offset params.

[x] Shared types and helpers
    [x] packages/core/src/queries/types.ts
        - PaginationParams (Zod schema: limit 1-100 default 50, offset >= 0)
        - PaginatedResult<T> interface (items, total, limit, offset)
        - Domain-specific filter schemas:
            duplicateGroupFilters: minConfidence, maxConfidence, reviewed,
              resolved, sortBy (confidence|created_at), sortOrder (asc|desc)
            documentFilters: correspondent, documentType, tag,
              processingStatus, search (title substring)
    [x] packages/core/src/queries/helpers.ts
        - paginate(query, params) helper for Drizzle
        - parseJsonColumn(value) for tags_json parsing

[x] Dashboard queries (packages/core/src/queries/dashboard.ts)
    [x] getDashboard(db) → DashboardData
        - Total document count
        - Total duplicate groups (unresolved)
        - Storage savings estimate (sum of archive_file_size for non-primary
          members of unresolved groups)
        - Documents pending analysis
        - Last sync date and document count
        - Last analysis date and group count
        - Top duplicated correspondents (correspondents with most duplicate groups)
    [x] GET /api/v1/dashboard — returns all dashboard data in one call

[x] Document queries (packages/core/src/queries/documents.ts)
    [x] getDocuments(db, filters, pagination) → PaginatedResult<DocumentSummary>
        - List documents with metadata (no OCR text in list view)
        - Filter by correspondent, document_type, tag, processing_status
        - Search by title substring
    [x] getDocument(db, id) → DocumentDetail | null
        - Full document with OCR text, word count, content hash
        - Include duplicate group memberships
    [x] GET /api/v1/documents — paginated document list
    [x] GET /api/v1/documents/:id — document detail

[x] Duplicate group queries (packages/core/src/queries/duplicates.ts)
    [x] getDuplicateGroups(db, filters, pagination) → PaginatedResult<DuplicateGroupSummary>
        - Group with member count, primary document title, confidence score
        - Filter by confidence range, reviewed/resolved status
        - Sort by confidence (desc by default) or created_at
    [x] getDuplicateGroup(db, id) → DuplicateGroupDetail | null
        - Full group with all members, their documents, and component scores
        - Include OCR text for each member (for side-by-side comparison)
    [x] getDuplicateStats(db) → DuplicateStats
        - Total groups, reviewed count, resolved count
        - Confidence distribution histogram (buckets: 85-90, 90-95, 95-100)
        - Top correspondents with duplicates
    [x] GET /api/v1/duplicates — paginated duplicate group list
    [x] GET /api/v1/duplicates/:id — duplicate group detail with member docs
    [x] GET /api/v1/duplicates/stats — duplicate statistics

[x] Duplicate group actions
    [x] PUT /api/v1/duplicates/:id/primary — set primary document in group
        - Accept { documentId: string }
        - Update is_primary flags within the group
    [x] PUT /api/v1/duplicates/:id/review — mark group as reviewed
    [x] PUT /api/v1/duplicates/:id/resolve — mark group as resolved
    [x] DELETE /api/v1/duplicates/:id — delete a duplicate group
        (removes the group record, does NOT delete documents)

[x] Batch operations
    [x] POST /api/v1/batch/review — mark multiple groups as reviewed
        - Accept { groupIds: string[] }
    [x] POST /api/v1/batch/resolve — mark multiple groups as resolved
        - Accept { groupIds: string[] }
    [x] POST /api/v1/batch/delete-non-primary — delete non-primary documents
        - Accept { groupIds: string[] }
        - Creates a batch_operation job
        - Worker iterates groups, calls PaperlessClient.deleteDocument() for
          each non-primary member
        - Marks groups as resolved after successful deletion
        - Records results in job.result_json (deleted count, errors)
        - THIS IS A DESTRUCTIVE OPERATION: require confirmation parameter
          { groupIds: [...], confirm: true }

[x] Configuration queries (packages/core/src/queries/config.ts)
    [x] getConfig(db) → Record<string, string>
        - Read all app_config rows as key-value pairs
    [x] setConfig(db, key, value) → void
        - Upsert app_config row
    [x] GET /api/v1/config — all configuration values
    [x] PUT /api/v1/config — update configuration values
        - Accept { key: string, value: string } or { settings: Record<string, string> }

[x] Unit tests for all query functions
    [x] Dashboard query tests
    [x] Document list/detail query tests
    [x] Duplicate group query tests with filtering and pagination
    [x] Config read/write tests
    [x] Batch operation validation tests

================================================================================
PHASE 6: UI — DASHBOARD & SETTINGS  [COMPLETED]
================================================================================

[x] Design system and theme tokens
    [x] Define a named color palette in Tailwind config (semantic names):
        - canvas: page background (warm neutral)
        - ink: primary text color (near-black)
        - accent: interactive elements, active states (blue/teal family)
        - ember: alerts, warnings, destructive actions (orange/red family)
        - soft: subtle borders, dividers, muted backgrounds
        - surface: card/panel backgrounds (white or very light)
        - muted: secondary text, placeholder text
        - success: positive actions, resolved states (green family)
    [x] Typography
        - Clean sans-serif for UI (Inter via @fontsource/inter-variable)
        - Monospace for IDs and hashes (JetBrains Mono via @fontsource/jetbrains-mono-variable)
        - Configure in Tailwind via fontFamily extension
    [x] Reusable CSS utility classes and Svelte components:
        - .panel (card with consistent padding, border, radius, shadow)
        - ConfidenceBadge: color-coded confidence score (green >95, yellow >85, red <85)
        - StatusBadge: reviewed/resolved/pending states
        - ProgressBar: job progress visualization
    [x] Document the design system in Tailwind config comments

[x] Dashboard page (packages/web/src/routes/+page.svelte)
    [x] +page.server.ts loads via getDashboard() from @paperless-dedupe/core
    [x] Summary stat cards:
        - Total documents synced
        - Duplicate groups found (unresolved)
        - Estimated storage savings
        - Documents pending analysis
    [x] Sync controls:
        - "Sync Now" button → POST /api/v1/sync
        - "Force Full Sync" option
        - Progress bar with SSE connection during sync
        - Last sync timestamp and document count
    [x] Analysis controls:
        - "Run Analysis" button → POST /api/v1/analysis
        - "Force Rebuild" option
        - Progress bar with SSE connection during analysis
        - Last analysis timestamp and results summary
    [x] Recent job history (last 5 jobs with status and duration)
    [x] Quick stats: top duplicated correspondents, confidence distribution

[x] Settings page (packages/web/src/routes/settings/+page.svelte)
    [x] Paperless-NGX connection settings
        - URL, API token (masked input), or username/password
        - "Test Connection" button → POST /api/v1/config/test-connection
        - Display connection status (connected/disconnected, version, doc count)
    [x] Deduplication parameters
        - Confidence threshold slider (50-100, default 85)
        - Weight sliders for each component (must sum to 100):
          Jaccard, Fuzzy, Metadata, Filename
        - MinHash permutations (advanced, default 192)
        - LSH bands (advanced, default 20)
        - Fuzzy matching toggle and sample size
        - Min OCR word count
        - Max OCR length
    [x] System information
        - Database dialect and path
        - Total documents, duplicate groups
        - Database file size (SQLite)
        - Application version

[x] Documents page (packages/web/src/routes/documents/+page.svelte)
    [x] Library statistics overview (not individual document list)
        - Total documents, OCR coverage, processing status breakdown
        - Correspondent distribution chart
        - Document type distribution chart
        - Tag cloud or tag frequency chart
    [x] Link to Paperless-NGX for individual document management

[x] Shared UI components
    [x] ProgressBar.svelte — animated progress with percentage and message
    [x] JobStatusCard.svelte — compact job display (type, status, duration)
    [x] ConfidenceBadge.svelte — color-coded score display
    [x] StatCard.svelte — KPI card with label, value, optional trend
    [x] EChart.svelte — ECharts wrapper component (install echarts)
